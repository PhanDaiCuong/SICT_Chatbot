{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21876bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random \n",
    "from tqdm import tqdm \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011329ef",
   "metadata": {},
   "source": [
    "## Tuyen sinh - Tin tuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d34236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pages:   2%|▏         | 1/50 [00:42<34:44, 42.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàn tất crawl.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m src \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src:\n\u001b[0;32m--> 112\u001b[0m     saved_filename \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m saved_filename:\n\u001b[1;32m    114\u001b[0m         images_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: src,\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_filename\u001b[39m\u001b[38;5;124m\"\u001b[39m: saved_filename,\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         })\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mdownload_image\u001b[0;34m(img_url, save_folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_folder, filename)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     19\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/site-packages/urllib3/response.py:1253\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp)\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1251\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39mhas_unconsumed_tail)\n\u001b[1;32m   1252\u001b[0m     ):\n\u001b[0;32m-> 1253\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m-> 1108\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39mhas_unconsumed_tail)\n\u001b[1;32m   1116\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/site-packages/urllib3/response.py:1024\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m   1021\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m-> 1024\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/site-packages/urllib3/response.py:1007\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/Agent/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- HÀM HỖ TRỢ ---\n",
    "def download_image(img_url, save_folder):\n",
    "    \"\"\"Tải ảnh và trả về tên file local\"\"\"\n",
    "    try: \n",
    "        # Thêm User-Agent để tránh bị server chặn request ảnh\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(img_url, headers=headers, stream=True, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Xử lý đuôi file\n",
    "            ext = img_url.split('.')[-1].split('?')[0]\n",
    "            if len(ext) > 4 or not ext: ext = \"jpg\"\n",
    "            \n",
    "            filename = f\"{uuid.uuid4()}.{ext}\"\n",
    "            file_path = os.path.join(save_folder, filename)\n",
    "            \n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return filename\n",
    "    except Exception as e:\n",
    "        # Chỉ in lỗi ngắn gọn để không làm rối màn hình\n",
    "        pass \n",
    "    return None\n",
    "\n",
    "# --- SETUP ---\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless=new') # Chạy ẩn\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "root_dir = './sict_corpus/tintuc'\n",
    "images_dir = os.path.join(root_dir, 'images')\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "n_pages = 50\n",
    "news_id = 0\n",
    "\n",
    "# Sử dụng Explicit Wait\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "try:\n",
    "    for page_idx in tqdm(range(1, n_pages + 1), desc=\"Pages\"):\n",
    "        try:\n",
    "            # 1. Vào trang danh sách\n",
    "            main_url = f'https://sict.haui.edu.vn/vn/tin-tuc/{page_idx}'\n",
    "            driver.get(main_url)\n",
    "            \n",
    "            # Lấy danh sách link (Dùng wait để đảm bảo list đã load)\n",
    "            news_lst_xpath = '//section[contains(@class, \"irs-blog-field\")]//h2/a'\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, news_lst_xpath)))\n",
    "            \n",
    "            news_tags = driver.find_elements(By.XPATH, news_lst_xpath)\n",
    "            # Lưu lại list URL để tránh lỗi StaleElement khi chuyển trang\n",
    "            news_page_urls = [tag.get_attribute('href') for tag in news_tags]\n",
    "\n",
    "            # 2. Vào từng bài viết\n",
    "            for news_page_url in news_page_urls:\n",
    "                try:\n",
    "                    driver.get(news_page_url)\n",
    "                    \n",
    "                    # Định vị Main Content\n",
    "                    main_content_xpath = '//section[contains(@class, \"irs-blog-field\")]//div[@class=\"col-md-8\"]'\n",
    "                    try:\n",
    "                        main_content_tag = wait.until(EC.presence_of_element_located((By.XPATH, main_content_xpath)))\n",
    "                    except:\n",
    "                        # Nếu không load được content, bỏ qua bài này\n",
    "                        continue\n",
    "\n",
    "                    # --- TRÍCH XUẤT DỮ LIỆU ---\n",
    "                    \n",
    "                    # Title\n",
    "                    title = \"\"\n",
    "                    try:\n",
    "                        title = main_content_tag.find_element(By.XPATH, './/p[@class=\"pTitle\"]').text.strip()\n",
    "                    except: pass\n",
    "\n",
    "                    # Abstract\n",
    "                    abstract = \"\"\n",
    "                    try:\n",
    "                        abstract = main_content_tag.find_element(By.XPATH, './/p[@class=\"pHead\"]').text.strip()\n",
    "                    except: pass\n",
    "\n",
    "                    # --- 4. Lấy Body Text VÀ Hình ảnh (Đã tối ưu Table/List) ---\n",
    "                    content_text = \"\"\n",
    "                    images_data = []\n",
    "                    \n",
    "                    try:\n",
    "                        # [CHIẾN LƯỢC QUAN TRỌNG]:\n",
    "                        # 1. Lấy h2, h3, ul, table (để xử lý cả bảng thay vì từng tr lẻ tẻ)\n",
    "                        # 2. Lấy p NHƯNG LOẠI TRỪ các p nằm trong ul hoặc table (để tránh lặp)\n",
    "                        xpath_query = (\n",
    "                            \".//h2 | .//h3 | .//ul | .//table | \"\n",
    "                            \".//p[not(ancestor::ul) and not(ancestor::table)]\"\n",
    "                        )\n",
    "                        \n",
    "                        content_tags = main_content_tag.find_elements(By.XPATH, xpath_query)\n",
    "                        \n",
    "                        text_parts = []\n",
    "                \n",
    "                        for tag in content_tags:\n",
    "                            tag_name = tag.tag_name.lower()\n",
    "                            \n",
    "                            # A. --- CHECK TRÙNG TITLE/ABSTRACT ---\n",
    "                            tag_class = tag.get_attribute(\"class\")\n",
    "                            if tag_name == 'p' and tag_class and (\"pTitle\" in tag_class or \"pHead\" in tag_class):\n",
    "                                continue\n",
    "                            \n",
    "                            # B. --- XỬ LÝ TEXT THEO LOẠI THẺ (MARKDOWN STYLE) ---\n",
    "                            text = \"\"\n",
    "                            \n",
    "                            if tag_name == 'h2':\n",
    "                                text = f\"## {tag.text.strip()}\"\n",
    "                                \n",
    "                            elif tag_name == 'h3':\n",
    "                                text = f\"### {tag.text.strip()}\"\n",
    "                                \n",
    "                            elif tag_name == 'ul':\n",
    "                                # Duyệt qua các thẻ li con để thêm gạch đầu dòng\n",
    "                                lis = tag.find_elements(By.TAG_NAME, \"li\")\n",
    "                                li_texts = [f\"- {li.text.strip()}\" for li in lis if li.text.strip()]\n",
    "                                text = \"\\n\".join(li_texts)\n",
    "                                \n",
    "                            elif tag_name == 'table':\n",
    "                                # Duyệt qua tr để làm bảng giả lập Markdown\n",
    "                                rows = tag.find_elements(By.TAG_NAME, \"tr\")\n",
    "                                table_lines = []\n",
    "                                for row in rows:\n",
    "                                    cols = row.find_elements(By.CSS_SELECTOR, \"td, th\")\n",
    "                                    # Nối các cột bằng dấu |\n",
    "                                    row_text = \" | \".join([c.text.strip().replace(\"\\n\", \" \") for c in cols])\n",
    "                                    table_lines.append(f\"| {row_text} |\")\n",
    "                                text = \"\\n\".join(table_lines)\n",
    "                                \n",
    "                            else: # Thẻ p\n",
    "                                text = tag.text.strip()\n",
    "\n",
    "                            if text:\n",
    "                                text_parts.append(text)\n",
    "                            \n",
    "                            # C. --- LẤY ẢNH (Logic cũ) ---\n",
    "                            # Tìm ảnh trong thẻ hiện tại (Block Scope)\n",
    "                            imgs_in_tag = tag.find_elements(By.TAG_NAME, \"img\")\n",
    "                            for img in imgs_in_tag:\n",
    "                                src = img.get_attribute('src')\n",
    "                                if src:\n",
    "                                    saved_filename = download_image(src, images_dir)\n",
    "                                    if saved_filename:\n",
    "                                        # Kiểm tra trùng ảnh trong list images_data (Optional)\n",
    "                                        if not any(d['original_url'] == src for d in images_data):\n",
    "                                            images_data.append({\n",
    "                                                \"original_url\": src,\n",
    "                                                \"local_filename\": saved_filename,\n",
    "                                                \"relative_path\": f\"images/{saved_filename}\"\n",
    "                                            })\n",
    "\n",
    "                        content_text = '\\n\\n'.join(text_parts) # Dùng 2 xuống dòng để tách đoạn rõ hơn\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Lỗi parse body: {e}\")\n",
    "\n",
    "                    # --- LƯU JSON ---\n",
    "                    article_data = {\n",
    "                        \"id\": f\"ttc_{news_id:03d}\",\n",
    "                        \"url\": news_page_url,\n",
    "                        \"title\": title,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"content\": content_text,\n",
    "                        \"images\": images_data \n",
    "                    }\n",
    "                    \n",
    "                    # --- LƯU FILE TXT ---\n",
    "                    final_content_lst = [title.upper(), abstract, content_text]\n",
    "                    final_content = '\\n\\n'.join([x for x in final_content_lst if x]) # Lọc bỏ phần tử rỗng\n",
    "\n",
    "                    news_filename_json = f\"news_sict_{news_id:03d}.json\"\n",
    "                    news_savepath_json = os.path.join(root_dir, news_filename_json)\n",
    "                    \n",
    "                    news_filename_txt = f\"news_sict_{news_id:03d}.txt\"\n",
    "                    news_savepath_txt = os.path.join(root_dir, news_filename_txt)\n",
    "                    \n",
    "                    with open(news_savepath_json, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "                    \n",
    "                    with open(news_savepath_txt, 'w', encoding='utf-8') as f:\n",
    "                        f.write(final_content)\n",
    "\n",
    "                    print(f\"--> Đã lưu xong: {news_filename_json} và {news_filename_txt}\")\n",
    "\n",
    "                except Exception as inner_e:\n",
    "                    print(f\"Lỗi xử lý url {news_page_url}: {inner_e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi trang danh sách {page_idx}: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Đảm bảo tắt trình duyệt khi xong hoặc gặp lỗi\n",
    "    driver.quit()\n",
    "    print(\"Hoàn tất crawl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec730e",
   "metadata": {},
   "source": [
    "## Khoa - Gioithieu - Phong/Trungtam - Daotao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6897f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý: https://sict.haui.edu.vn/vn/html/khoa-hoc-may-tinh\n",
      "--> Đã lưu xong: news_sict_000.json và news_sict_000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# --- HÀM HỖ TRỢ ---\n",
    "def download_image(img_url, save_folder):\n",
    "    try: \n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(img_url, headers=headers, stream=True, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            ext = img_url.split('.')[-1].split('?')[0]\n",
    "            if len(ext) > 4 or not ext: ext = \"jpg\"\n",
    "            filename = f\"{uuid.uuid4()}.{ext}\"\n",
    "            file_path = os.path.join(save_folder, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return filename\n",
    "    except:\n",
    "        pass \n",
    "    return None\n",
    "\n",
    "# --- SETUP ---\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless=new')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "root_dir = './sict_corpus/gioithieu/ttc'\n",
    "images_dir = os.path.join(root_dir, 'images')\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "target_urls = [\n",
    "    'https://sict.haui.edu.vn/vn/html/khoa-hoc-may-tinh'\n",
    "]\n",
    "\n",
    "try:\n",
    "    for idx, url in enumerate(target_urls):\n",
    "        try:\n",
    "            print(f\"Đang xử lý: {url}\")\n",
    "            driver.get(url)\n",
    "\n",
    "            # 1. Định vị Main Content\n",
    "            main_content_xpath = '//section[contains(@class, \"irs-blog-field\")]//div[@class=\"col-md-8\"]'\n",
    "            try:\n",
    "                main_content_tag = wait.until(EC.presence_of_element_located((By.XPATH, main_content_xpath)))\n",
    "            except:\n",
    "                print(f\"Không tìm thấy nội dung chính tại {url}\")\n",
    "                continue\n",
    "\n",
    "            # --- TRÍCH XUẤT DỮ LIỆU ---\n",
    "\n",
    "            # 2. Lấy Title\n",
    "            title = \"\"\n",
    "            try:\n",
    "                title = main_content_tag.find_element(By.XPATH, './/p[@class=\"pTitle\"]').text.strip()\n",
    "            except: pass\n",
    "\n",
    "            # 3. Lấy Abstract\n",
    "            abstract = \"\"\n",
    "            try:\n",
    "                abstract = main_content_tag.find_element(By.XPATH, './/p[@class=\"pHead\"]').text.strip()\n",
    "            except: pass\n",
    "\n",
    "            # 4. Lấy Body Text VÀ Hình ảnh\n",
    "            content_text = \"\"\n",
    "            images_data = []\n",
    "            \n",
    "            try:\n",
    "                # [SỬA LỖI] Dùng CSS Selector để lấy p, h2, h3 cùng lúc\n",
    "                # Dấu phẩy nghĩa là \"hoặc\" (lấy p HOẶC h2 HOẶC h3)\n",
    "                content_tags = main_content_tag.find_elements(By.CSS_SELECTOR, \"p, h2, h3, ul, tr\")\n",
    "                \n",
    "                text_parts = []\n",
    "                \n",
    "                for tag in content_tags:\n",
    "                    # --- KIỂM TRA ĐỂ TRÁNH LẶP ---\n",
    "                    tag_class = tag.get_attribute(\"class\")\n",
    "                    if tag_class and (\"pTitle\" in tag_class or \"pHead\" in tag_class):\n",
    "                        continue\n",
    "                    \n",
    "                    # 1. Lấy Text và Format theo thẻ\n",
    "                    text = tag.text.strip()\n",
    "                    if text:\n",
    "                        # Nếu là Header thì thêm dấu # để file txt đẹp hơn (Markdown style)\n",
    "                        if tag.tag_name == 'h2':\n",
    "                            text_parts.append(f\"\\n## {text}\")\n",
    "                        elif tag.tag_name == 'h3':\n",
    "                            text_parts.append(f\"\\n### {text}\")\n",
    "                        else:\n",
    "                            text_parts.append(text)\n",
    "                    \n",
    "                    # 2. Lấy Ảnh (tìm trong tag hiện tại)\n",
    "                    imgs_in_tag = tag.find_elements(By.TAG_NAME, \"img\")\n",
    "                    for img in imgs_in_tag:\n",
    "                        src = img.get_attribute('src')\n",
    "                        if src:\n",
    "                            saved_filename = download_image(src, images_dir)\n",
    "                            if saved_filename:\n",
    "                                images_data.append({\n",
    "                                    \"original_url\": src,\n",
    "                                    \"local_filename\": saved_filename,\n",
    "                                    \"relative_path\": f\"images/{saved_filename}\"\n",
    "                                })\n",
    "\n",
    "                content_text = '\\n'.join(text_parts)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi parse body: {e}\")\n",
    "\n",
    "            # --- LƯU JSON ---\n",
    "            article_data = {\n",
    "                \"id\": f\"ttc_{idx:03d}\",\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"content\": content_text,\n",
    "                \"images\": images_data \n",
    "            }\n",
    "            \n",
    "            # --- LƯU FILE TXT ---\n",
    "            final_content_lst = [title.upper(), abstract, content_text]\n",
    "            final_content = '\\n\\n'.join([x for x in final_content_lst if x]) # Lọc bỏ phần tử rỗng\n",
    "\n",
    "            news_filename_json = f\"news_sict_{idx:03d}.json\"\n",
    "            news_savepath_json = os.path.join(root_dir, news_filename_json)\n",
    "            \n",
    "            news_filename_txt = f\"news_sict_{idx:03d}.txt\"\n",
    "            news_savepath_txt = os.path.join(root_dir, news_filename_txt)\n",
    "            \n",
    "            with open(news_savepath_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            with open(news_savepath_txt, 'w', encoding='utf-8') as f:\n",
    "                f.write(final_content)\n",
    "\n",
    "            print(f\"--> Đã lưu xong: {news_filename_json} và {news_filename_txt}\")\n",
    "\n",
    "        except Exception as inner_e:\n",
    "            print(f\"Lỗi xử lý url {url}: {inner_e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe62a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
