import os
import json
import logging
import sys
from pathlib import Path
from uuid import uuid4
from tqdm import tqdm
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker
from qdrant_client import QdrantClient, models
from dotenv import load_dotenv
from utils.config import PATH_MAPPING, CLASSIFICATION_SETS


# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LOGGER = logging.getLogger(__name__)


# ==========================================
# 1. H√ÄM X·ª¨ L√ù NG·ªÆ C·∫¢NH (CONTEXT LOGIC)
# ==========================================
def generate_dynamic_context(file_path_str):
    """
    Ph√¢n t√≠ch ƒë∆∞·ªùng d·∫´n file ƒë·ªÉ t·∫°o ng·ªØ c·∫£nh t·ª± nhi√™n v√† metadata ph√¢n lo·∫°i.
    """
    path = Path(file_path_str)
    parts = path.parts
    
    metadata = {
        "school": None,
        "major": None,
        "department": None,
        "level": None,
        "topics": [],
        "raw_path": str(file_path_str) # L∆∞u ƒë∆∞·ªùng d·∫´n g·ªëc ƒë·ªÉ debug
    }
    
    context_keywords = []

    for part in parts:
        part_key = part.lower()
        
        # Ch·ªâ x·ª≠ l√Ω n·∫øu key c√≥ trong t·ª´ ƒëi·ªÉn Mapping
        if part_key in PATH_MAPPING:
            human_text = PATH_MAPPING[part_key]
            
            # --- Logic Ph√¢n Lo·∫°i ---
            if part_key in CLASSIFICATION_SETS["SCHOOLS"]:
                metadata["school"] = part_key
                context_keywords.append(f"ƒê∆°n v·ªã: {human_text}")
                
            elif part_key in CLASSIFICATION_SETS["MAJORS"]:
                metadata["major"] = part_key
                context_keywords.append(f"Ng√†nh: {human_text}")
                
            elif part_key in CLASSIFICATION_SETS["DEPARTMENTS"]:
                metadata["department"] = part_key
                context_keywords.append(f"ƒê∆°n v·ªã tr·ª±c thu·ªôc: {human_text}")
                
            elif part_key in CLASSIFICATION_SETS["LEVELS"]:
                metadata["level"] = part_key
                context_keywords.append(f"H·ªá ƒë√†o t·∫°o: {human_text}")
                
            else:
                metadata["topics"].append(part_key)
                context_keywords.append(f"M·ª•c: {human_text}")

    # T·∫°o c√¢u Context ƒë·ªÉ ti√™m v√†o n·ªôi dung
    if context_keywords:
        full_context_str = " - ".join(context_keywords) + "."
    else:
        full_context_str = "Th√¥ng tin chung ƒê·∫°i h·ªçc C√¥ng nghi·ªáp H√† N·ªôi."
        
    # L√†m g·ªçn metadata topics
    metadata["topics"] = ", ".join(metadata["topics"]) if metadata["topics"] else None

    return full_context_str, metadata

# ==========================================
# 2. H√ÄM LOAD & CHUNK D·ªÆ LI·ªÜU
# ==========================================
def process_file_semantic(file_path: Path, text_splitter) -> list[Document]:
    """
    ƒê·ªçc 1 file, ti√™m ng·ªØ c·∫£nh, v√† chia nh·ªè b·∫±ng Semantic Chunking.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Ki·ªÉm tra format JSON c∆° b·∫£n
        # Gi·∫£ s·ª≠ file json ch·ª©a 1 object {title, abstract, content...}
        # N·∫øu json l√† list c√°c b√†i vi·∫øt, c·∫ßn v√≤ng l·∫∑p for ·ªü ƒë√¢y.
        if isinstance(data, list):
            # N·∫øu file json ch·ª©a list, ta x·ª≠ l√Ω b√†i ƒë·∫ßu ti√™n ho·∫∑c loop (t√πy c·∫•u tr√∫c data c·ªßa b·∫°n)
            # ·ªû ƒë√¢y gi·∫£ ƒë·ªãnh 1 file = 1 b√†i vi·∫øt ƒë·ªÉ t·ªëi ∆∞u ng·ªØ c·∫£nh folder
            data = data[0] if data else {}

        # 1. L·∫•y ng·ªØ c·∫£nh t·ª´ ƒë∆∞·ªùng d·∫´n
        context_str, path_metadata = generate_dynamic_context(str(file_path))

        # 2. Chu·∫©n b·ªã n·ªôi dung th√¥ (Raw Text) v·ªõi Context Injection
        # ƒê∆∞a Context l√™n ƒë·∫ßu ƒë·ªÉ Semantic Model hi·ªÉu ngay ng·ªØ c·∫£nh
        title = data.get('title', '')
        abstract = data.get('abstract', '')
        content = data.get('content', '')
        
        # Format text ƒë·ªÉ chunking
        raw_text = f"{context_str}\n\nTi√™u ƒë·ªÅ: {title}\n\nT√≥m t·∫Øt: {abstract}\n\nN·ªôi dung: {content}"

        # 3. Chu·∫©n b·ªã Metadata g·ªëc t·ª´ file
        file_metadata = {
            "title": title,
            "url": data.get("url", ""),
            "id": data.get("id", str(uuid4())),
            "image_url": data.get("images", [{}])[0].get("original_url") if data.get("images") else None
        }
        
        # Merge metadata t·ª´ Path v√† metadata t·ª´ File
        final_metadata = {**path_metadata, **file_metadata}

        # 4. Th·ª±c hi·ªán Semantic Chunking
        docs = text_splitter.create_documents([raw_text], metadatas=[final_metadata])
        
        return docs

    except Exception as e:
        LOGGER.error(f"Failed to process file {file_path}: {e}")
        return []

# ==========================================
# 3. H√ÄM QU·∫¢N L√ù K·∫æT N·ªêI QDRANT
# ==========================================
def init_qdrant_collection(client: QdrantClient, collection_name: str):
    """ƒê·∫£m b·∫£o Collection t·ªìn t·∫°i v·ªõi config ƒë√∫ng."""
    collections = client.get_collections().collections
    exists = any(c.name == collection_name for c in collections)

    if not exists:
        LOGGER.info(f"Creating collection '{collection_name}'...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=1536,  
                distance=models.Distance.COSINE
            )
        )
    else:
        LOGGER.info(f"Collection '{collection_name}' already exists.")

# ==========================================
# 4. MAIN SEEDING FUNCTION (BATCH PROCESSING)
# ==========================================
def seed_qdrant_recursive(
    root_dir: str, 
    qdrant_host: str, 
    qdrant_api_key: str, 
    collection_name: str, 
    openai_api_key: str
):
    # 1. Setup Models
    embedding_model = OpenAIEmbeddings(
        api_key=openai_api_key,
        model="text-embedding-3-small" # Khuy√™n d√πng model n√†y thay v√¨ ada-002 (r·∫ª h∆°n & t·ªët h∆°n)
    )

    # Semantic Chunker setup
    # breakpoint_threshold_type="percentile": C·∫Øt d·ª±a tr√™n s·ª± thay ƒë·ªïi ng·ªØ nghƒ©a ƒë·ªôt ng·ªôt
    text_splitter = SemanticChunker(
        embedding_model,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=90 # Ng∆∞·ª°ng nh·∫°y (90-95 l√† t·ªët cho vƒÉn b·∫£n tin t·ª©c)
    )

    # 2. Setup Qdrant Client
    client = QdrantClient(url=qdrant_host, api_key=qdrant_api_key)
    init_qdrant_collection(client, collection_name)
    
    # K·∫øt n·ªëi VectorStore
    vectorstore = Qdrant(
        client=client,
        collection_name=collection_name,
        embeddings=embedding_model
    )

    # 3. Qu√©t to√†n b·ªô file JSON (Recursive)
    root_path = Path(root_dir)
    json_files = list(root_path.rglob("*.json"))
    LOGGER.info(f"Found {len(json_files)} JSON files in {root_dir}")

    # 4. X·ª≠ l√Ω & Upload theo Batch (ƒê·ªÉ ti·∫øt ki·ªám RAM)
    BATCH_SIZE = 50 # S·ªë l∆∞·ª£ng chunks s·∫Ω upload 1 l·∫ßn
    chunk_buffer = []
    
    # D√πng tqdm ƒë·ªÉ hi·ªán thanh loading
    for file_path in tqdm(json_files, desc="Processing Files"):
        
        # X·ª≠ l√Ω t·ª´ng file -> ra nhi·ªÅu chunks
        chunks = process_file_semantic(file_path, text_splitter)
        
        # Th√™m v√†o b·ªô ƒë·ªám
        chunk_buffer.extend(chunks)

        # N·∫øu b·ªô ƒë·ªám ƒë·∫ßy th√¨ ƒë·∫©y l√™n Qdrant
        if len(chunk_buffer) >= BATCH_SIZE:
            vectorstore.add_documents(chunk_buffer)
            chunk_buffer = [] # Clear buffer

    # ƒê·∫©y n·ªët s·ªë c√≤n d∆∞
    if chunk_buffer:
        vectorstore.add_documents(chunk_buffer)

    LOGGER.info("Seeding completed successfully!")

# ==========================================
# 5. ENTRY POINT
# ==========================================
if __name__ == "__main__":
    load_dotenv()
    
    # Load Env Variables
    QDRANT_HOST = os.getenv("QDRANT_HOST", "http://localhost:6333")
    QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")
    QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "sict_documents_semantic")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    # ƒê∆∞·ªùng d·∫´n data g·ªëc (N∆°i ch·ª©a folder sict_corpus, seee_corpus...)
    # L∆∞u √Ω: Tr·ªè v√†o folder cha ch·ª©a c√°c corpus
    current_dir = Path(__file__).parent
    DATA_DIRECTORY = os.getenv("DATA_DIR", str(current_dir.parent.parent / "data"))

    if not OPENAI_API_KEY:
        LOGGER.error("OPENAI_API_KEY is missing!")
        sys.exit(1)

    if not os.path.exists(DATA_DIRECTORY):
        LOGGER.error(f"Data directory not found: {DATA_DIRECTORY}")
        sys.exit(1)

    print("\nüöÄ STARTING SEMANTIC SEEDING PIPELINE üöÄ")
    print(f"Target: {QDRANT_HOST} | Collection: {QDRANT_COLLECTION}")
    print(f"Scanning Data: {DATA_DIRECTORY}")
    
    try:
        seed_qdrant_recursive(
            root_dir=DATA_DIRECTORY,
            qdrant_host=QDRANT_HOST,
            qdrant_api_key=QDRANT_API_KEY,
            collection_name=QDRANT_COLLECTION,
            openai_api_key=OPENAI_API_KEY
        )
    except KeyboardInterrupt:
        print("\nStopped by user.")
    except Exception as e:
        LOGGER.critical(f"Fatal Error: {e}")