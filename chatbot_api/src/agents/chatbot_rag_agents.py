import os
import logging
import traceback
from dotenv import load_dotenv

# LangChain / LangGraph Imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool as lc_tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition

# Qdrant / Embeddings Imports
from qdrant_client import QdrantClient
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.documents import Document

# Custom Imports (ƒê·∫£m b·∫£o c√°c file n√†y t·ªìn t·∫°i)
from .tools.chatbot_retriever_tool import build_optimized_rag_pipeline, BM25IndexManager
from .prompt.system_prompt import system

# --- C·∫§U H√åNH LOGGING ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Lumiya-Core")

load_dotenv()

# --- CONFIG ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
QDRANT_HOST = os.getenv('QDRANT_HOST', 'http://localhost:6333')
QDRANT_COLLECTION = os.getenv('COLLECTION_NAME') # Quan tr·ªçng: Ph·∫£i kh·ªõp v·ªõi l√∫c n·∫°p d·ªØ li·ªáu

DB_CONFIG = {
    "host": os.getenv('HOST'),
    "user": 'root',
    "password": os.getenv('MYSQL_ROOT_PASSWORD'),
    "database": os.getenv('MYSQL_DATABASE'),
}
logger.info(f"-------------------->Th√¥ng tin k·∫øt n·ªëi mysql: {DB_CONFIG}")

# --- 1. KH·ªûI T·∫†O COMPONENTS ---
def initialize_components():
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY is missing!")
    
    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)
    
    # K·∫øt n·ªëi Qdrant
    client = QdrantClient(url=QDRANT_HOST, api_key=os.getenv('QDRANT_API_KEY'))
    
    # Init Vector Store
    # L∆∞u √Ω: N·∫øu Collection ch∆∞a t·ªìn t·∫°i, d√≤ng n√†y c√≥ th·ªÉ g√¢y l·ªói. 
    # ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y main.py ƒë·ªÉ t·∫°o collection tr∆∞·ªõc.
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=QDRANT_COLLECTION,
        embedding=embedding_model,
    )
    
    chat_model = ChatOpenAI(model=OPENAI_MODEL_NAME, temperature=0, api_key=OPENAI_API_KEY)
    return embedding_model, vector_store, chat_model

# Ch·∫°y kh·ªüi t·∫°o
embedding_model, vector_store, chat_model = initialize_components()

# --- 2. X√ÇY D·ª∞NG RETRIEVER PIPELINE & TOOL ---
tools = [] # M·∫∑c ƒë·ªãnh l√† r·ªóng

if vector_store:
    try:
        bm25_path = os.getenv("BM25_INDEX_PATH", "bm25_index.pkl")
        force_rebuild = os.getenv("BM25_FORCE_REBUILD", "false").lower() in ("true", "1", "t")
        
        # Logic 1: L·∫•y documents t·ª´ Qdrant ƒë·ªÉ build BM25 (n·∫øu c·∫ßn)
        documents_for_bm25 = None
        
        # Ch·ªâ l·∫•y d·ªØ li·ªáu n·∫øu c·∫ßn rebuild ho·∫∑c file ch∆∞a t·ªìn t·∫°i
        if force_rebuild or not os.path.exists(bm25_path):
            logger.info("‚ö†Ô∏è ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Qdrant ƒë·ªÉ build BM25 Index (L·∫ßn ƒë·∫ßu ho·∫∑c Force Rebuild)...")
            limit_k = int(os.getenv("BM25_CORPUS_K", "5000"))
            
            # Scroll l·∫•y d·ªØ li·ªáu th√¥
            response_scroll, _ = vector_store.client.scroll(
                collection_name=QDRANT_COLLECTION,
                limit=limit_k,
                with_payload=True
            )
            
            # Chuy·ªÉn ƒë·ªïi sang Document object
            documents_for_bm25 = [
                Document(page_content=p.payload.get("page_content", ""), metadata=p.payload.get("metadata", {}))
                for p in response_scroll if p.payload and "page_content" in p.payload
            ]
            logger.info(f"ƒê√£ t·∫£i {len(documents_for_bm25)} documents cho BM25.")

        # Logic 2: Build Pipeline (Hybrid Search + Rerank)
        # L∆∞u √Ω: H√†m build_optimized_rag_pipeline b√™n file kia ph·∫£i ch·∫•p nh·∫≠n tham s·ªë bm25_manager
        retriever = build_optimized_rag_pipeline(
            vector_store=vector_store,
            bm25_manager=BM25IndexManager(bm25_path), 
            documents_for_bm25=documents_for_bm25,
            force_rebuild_bm25=force_rebuild,
        )

        # Logic 3: ƒê·ªãnh nghƒ©a Tool (C√≥ Try/Catch an to√†n)
        @lc_tool("Search_HaUI_Info")
        def search_haui_info(query: str) -> str:
            """Tra c·ª©u th√¥ng tin ch√≠nh th·ª©c v·ªÅ HaUI (Nh√¢n s·ª±, h·ªçc ph√≠, l·ªãch thi, tin t·ª©c, quy ch·∫ø...). 
            C·∫ßn s·ª≠ d·ª•ng c√¥ng c·ª• n√†y tr∆∞·ªõc khi k·∫øt lu·∫≠n kh√¥ng c√≥ d·ªØ li·ªáu."""
            
            logger.info(f"[START] ƒêang truy xu·∫•t d·ªØ li·ªáu cho query: '{query}'")
            try:
                results = retriever.invoke(query)
            except Exception as e:
                logger.error(f" [ERROR] L·ªói nghi√™m tr·ªçng khi g·ªçi Retriever!")
                return f"H·ªá th·ªëng g·∫∑p l·ªói k·ªπ thu·∫≠t khi tra c·ª©u: {str(e)}"

            if not results:
                logger.warning(f" [WARN] Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho: {query}")
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."
            
            logger.info(f"[SUCCESS] T√¨m th·∫•y {len(results)} t√†i li·ªáu.")
            first_doc_snippet = results[0].page_content[:100].replace('\n', ' ')
            logger.info(f"Preview: {first_doc_snippet}...")
            
            return "\n\n".join([f"Ngu·ªìn: {d.page_content}" for d in results])

        # --- QUAN TR·ªåNG: ƒêƒÉng k√Ω tool v√†o list ---
        tools = [search_haui_info]
        logger.info(" ƒê√£ kh·ªüi t·∫°o Tool: Search_HaUI_Info")

    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o Retriever/Tool: {e}")
        logger.error(traceback.format_exc())
        # N·∫øu l·ªói, tools v·∫´n l√† [] ƒë·ªÉ tr√°nh s·∫≠p app, nh∆∞ng Agent s·∫Ω kh√¥ng t√¨m ki·∫øm ƒë∆∞·ª£c.


# --- 3. LANGGRAPH LOGIC ---
if chat_model:
    # Bind tools v√†o model
    model_with_tools = chat_model.bind_tools(tools)

    def call_model(state: MessagesState):
        msgs = state["messages"]
        
        # System Prompt
        sys_msg = SystemMessage(content=system)
        
        # Clean up old system messages
        filtered_msgs = [m for m in msgs if not isinstance(m, SystemMessage)]
        input_msgs = [sys_msg] + filtered_msgs

        # G·ªçi LLM
        response = model_with_tools.invoke(input_msgs)
        
        # Log h√†nh vi Agent
        if response.tool_calls:
            logger.info(f"üõ†Ô∏è Agent quy·∫øt ƒë·ªãnh g·ªçi Tool: {response.tool_calls[0]['name']}")
        else:
            logger.info("üß† Agent ph·∫£n h·ªìi tr·ª±c ti·∫øp (Kh√¥ng d√πng Tool).")
            
        return {"messages": [response]}

    # Graph Setup
    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", ToolNode(tools))

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", tools_condition)
    workflow.add_edge("tools", "agent")

    app = workflow.compile()

    # --- ADAPTER (Gi·ªØ nguy√™n ƒë·ªÉ t∆∞∆°ng th√≠ch API c≈©) ---
    class GraphAgentExecutorAdapter:
        def __init__(self, graph):
            self.graph = graph

        def invoke(self, inputs: dict):
            # Chuy·ªÉn ƒë·ªïi chat_history t·ª´ dict sang object LangChain
            msg_list = []
            for m in inputs.get("chat_history", []):
                if m.get("type") == "user":
                    msg_list.append(HumanMessage(content=m.get("content")))
                else:
                    msg_list.append(AIMessage(content=m.get("content")))
            
            if inputs.get("input"):
                msg_list.append(HumanMessage(content=inputs.get("input")))

            result = self.graph.invoke({"messages": msg_list})
            # L·∫•y tin nh·∫Øn AI cu·ªëi c√πng
            final_ai_msg = [m for m in result["messages"] if isinstance(m, AIMessage)][-1]
            return {"output": final_ai_msg.content}

    chatbot_agent_executor = GraphAgentExecutorAdapter(app)
else:
    logger.critical("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Agent do thi·∫øu Chat Model!")
    chatbot_agent_executor = None