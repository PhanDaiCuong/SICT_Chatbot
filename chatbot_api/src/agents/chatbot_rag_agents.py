"""
RAG Agent setup using LangChain + LangGraph with hybrid retrieval tools.

This module wires together:
- OpenAI chat model for response generation.
- Qdrant vector store for semantic search.
- A custom retrieval tool (`Search_HaUI_Info`) built on an optimized pipeline.
- A LangGraph workflow that decides when to call tools.
- A small adapter that exposes a simple `.invoke({...})` API.

Environment Variables
---------------------
Required:
- `OPENAI_API_KEY`: API key for the OpenAI model.

Optional (with defaults):
- `OPENAI_MODEL` (default: `gpt-4o-mini`): Chat model name.
- `EMBEDDING_MODEL` (default: `text-embedding-3-small`): Embedding model.
- `QDRANT_HOST` (default: `http://localhost:6333`): Qdrant endpoint.
- `COLLECTION_NAME`: Name of the Qdrant collection (must exist ahead of time).
- `QDRANT_API_KEY`: If your Qdrant instance requires authentication.
- `BM25_INDEX_PATH` (default: `bm25_index.pkl`): Path to cached BM25 index.
- `BM25_FORCE_REBUILD` (default: `false`): Force rebuild the BM25 index.
- `BM25_CORPUS_K` (default: `5000`): Max payloads to pull when building BM25.

Database (optional, currently logged for visibility):
- `HOST`, `MYSQL_ROOT_PASSWORD`, `MYSQL_DATABASE` (see `DB_CONFIG`).

Quick Start
-----------
1) Ensure Qdrant is running and the collection is created (via your ingestion).
2) Set environment variables (e.g., via `.env`).
3) Import `chatbot_agent_executor` and call `.invoke({"input": "..."})`.

Example
-------
````python
from chatbot_api.src.agents.chatbot_rag_agents import chatbot_agent_executor

result = chatbot_agent_executor.invoke({
    "chat_history": [{"type": "user", "content": "Hello"}],
    "input": "What is the admission deadline?"
})
print(result["output"])  # Final assistant response
````
"""

import os
import logging
import traceback
from dotenv import load_dotenv

# LangChain / LangGraph Imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool as lc_tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition

# Qdrant / Embeddings Imports
from qdrant_client import QdrantClient
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.documents import Document

# Custom Imports (ƒê·∫£m b·∫£o c√°c file n√†y t·ªìn t·∫°i)
from .tools.chatbot_retriever_tool import build_optimized_rag_pipeline, BM25IndexManager
from .prompt.system_prompt import system

# --- C·∫§U H√åNH LOGGING ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Lumiya-Core")

load_dotenv()

# --- CONFIG ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
QDRANT_HOST = os.getenv('QDRANT_HOST', 'http://localhost:6333')
QDRANT_COLLECTION = os.getenv('COLLECTION_NAME') # Quan tr·ªçng: Ph·∫£i kh·ªõp v·ªõi l√∫c n·∫°p d·ªØ li·ªáu

DB_CONFIG = {
    "host": os.getenv('HOST'),
    "user": 'root',
    "password": os.getenv('MYSQL_ROOT_PASSWORD'),
    "database": os.getenv('MYSQL_DATABASE'),
}
logger.info(f"-------------------->Th√¥ng tin k·∫øt n·ªëi mysql: {DB_CONFIG}")

# --- 1. KH·ªûI T·∫†O COMPONENTS ---
def initialize_components():
    """Initialize embeddings, Qdrant vector store, and chat model.

    Returns
    -------
    tuple
        A triple `(embedding_model, vector_store, chat_model)`.

    Raises
    ------
    ValueError
        If `OPENAI_API_KEY` is missing.

    Notes
    -----
    - Ensure your Qdrant collection exists; otherwise the vector store
      initialization may fail. Run your ingestion beforehand.
    - `ChatOpenAI` is created with low temperature for deterministic outputs.
    """
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY is missing!")
    
    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)
    
    # K·∫øt n·ªëi Qdrant
    client = QdrantClient(url=QDRANT_HOST, api_key=os.getenv('QDRANT_API_KEY'))
    
    # Init Vector Store
    # L∆∞u √Ω: N·∫øu Collection ch∆∞a t·ªìn t·∫°i, d√≤ng n√†y c√≥ th·ªÉ g√¢y l·ªói. 
    # ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y main.py ƒë·ªÉ t·∫°o collection tr∆∞·ªõc.
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=QDRANT_COLLECTION,
        embedding=embedding_model,
    )
    
    chat_model = ChatOpenAI(model=OPENAI_MODEL_NAME, temperature=0, api_key=OPENAI_API_KEY)
    return embedding_model, vector_store, chat_model

# Ch·∫°y kh·ªüi t·∫°o
embedding_model, vector_store, chat_model = initialize_components()

# --- 2. X√ÇY D·ª∞NG RETRIEVER PIPELINE & TOOL ---
tools = [] # M·∫∑c ƒë·ªãnh l√† r·ªóng

if vector_store:
    try:
        bm25_path = os.getenv("BM25_INDEX_PATH", "bm25_index.pkl")
        force_rebuild = os.getenv("BM25_FORCE_REBUILD", "false").lower() in ("true", "1", "t")
        
        # Logic 1: L·∫•y documents t·ª´ Qdrant ƒë·ªÉ build BM25 (n·∫øu c·∫ßn)
        documents_for_bm25 = None
        
        # Ch·ªâ l·∫•y d·ªØ li·ªáu n·∫øu c·∫ßn rebuild ho·∫∑c file ch∆∞a t·ªìn t·∫°i
        if force_rebuild or not os.path.exists(bm25_path):
            logger.info("‚ö†Ô∏è ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Qdrant ƒë·ªÉ build BM25 Index (L·∫ßn ƒë·∫ßu ho·∫∑c Force Rebuild)...")
            limit_k = int(os.getenv("BM25_CORPUS_K", "5000"))
            
            # Scroll l·∫•y d·ªØ li·ªáu th√¥
            response_scroll, _ = vector_store.client.scroll(
                collection_name=QDRANT_COLLECTION,
                limit=limit_k,
                with_payload=True
            )
            
            # Chuy·ªÉn ƒë·ªïi sang Document object
            documents_for_bm25 = [
                Document(page_content=p.payload.get("page_content", ""), metadata=p.payload.get("metadata", {}))
                for p in response_scroll if p.payload and "page_content" in p.payload
            ]
            logger.info(f"ƒê√£ t·∫£i {len(documents_for_bm25)} documents cho BM25.")

        # Logic 2: Build Pipeline (Hybrid Search + Rerank)
        retriever = build_optimized_rag_pipeline(
            vector_store=vector_store,
            bm25_manager=BM25IndexManager(bm25_path), 
            documents_for_bm25=documents_for_bm25,
            force_rebuild_bm25=force_rebuild,
        )

        # Logic 3: ƒê·ªãnh nghƒ©a Tool (C√≥ Try/Catch an to√†n)
        @lc_tool("Search_HaUI_Info")
        def search_haui_info(query: str) -> str:
            """Search official HaUI information via the hybrid RAG retriever.

            Use this tool to fetch authoritative information (staff, tuition,
            schedules, news, regulations, etc.). Prefer calling this before
            concluding that data is unavailable.

            Parameters
            ----------
            query : str
                Natural language query to search in the indexed corpus.

            Returns
            -------
            str
                A newline-separated string of sources concatenating page content.
                If empty or an error occurs, returns a descriptive message.

            Notes
            -----
            - Logs success and a small preview of the first result.
            - Catches exceptions to keep the agent responsive.
            """
            """Tra c·ª©u th√¥ng tin ch√≠nh th·ª©c v·ªÅ HaUI (Nh√¢n s·ª±, h·ªçc ph√≠, l·ªãch thi, tin t·ª©c, quy ch·∫ø...). 
            C·∫ßn s·ª≠ d·ª•ng c√¥ng c·ª• n√†y tr∆∞·ªõc khi k·∫øt lu·∫≠n kh√¥ng c√≥ d·ªØ li·ªáu."""
            
            logger.info(f"[START] ƒêang truy xu·∫•t d·ªØ li·ªáu cho query: '{query}'")
            try:
                results = retriever.invoke(query)
            except Exception as e:
                logger.error(f" [ERROR] L·ªói nghi√™m tr·ªçng khi g·ªçi Retriever!")
                return f"H·ªá th·ªëng g·∫∑p l·ªói k·ªπ thu·∫≠t khi tra c·ª©u: {str(e)}"

            if not results:
                logger.warning(f" [WARN] Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho: {query}")
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."
            
            logger.info(f"[SUCCESS] T√¨m th·∫•y {len(results)} t√†i li·ªáu.")
            first_doc_snippet = results[0].page_content[:100].replace('\n', ' ')
            logger.info(f"Preview: {first_doc_snippet}...")
            
            return "\n\n".join([f"Ngu·ªìn: {d.page_content}" for d in results])

        # --- QUAN TR·ªåNG: ƒêƒÉng k√Ω tool v√†o list ---
        tools = [search_haui_info]
        logger.info(" ƒê√£ kh·ªüi t·∫°o Tool: Search_HaUI_Info")

    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o Retriever/Tool: {e}")
        logger.error(traceback.format_exc())
        # N·∫øu l·ªói, tools v·∫´n l√† [] ƒë·ªÉ tr√°nh s·∫≠p app, nh∆∞ng Agent s·∫Ω kh√¥ng t√¨m ki·∫øm ƒë∆∞·ª£c.


# --- 3. LANGGRAPH LOGIC ---
if chat_model:
    # Bind tools v√†o model
    model_with_tools = chat_model.bind_tools(tools)

    def call_model(state: MessagesState):
        """Core node: build input messages, call the LLM, and return response.

        Steps
        -----
        - Prepend the `system` prompt to incoming messages.
        - Invoke the model (with tools bound) and capture its response.
        - Log whether the agent decided to use a tool.

        Parameters
        ----------
        state : MessagesState
            The current message state managed by LangGraph.

        Returns
        -------
        dict
            A dict with a `messages` key containing the latest assistant message.
        """
        msgs = state["messages"]
        
        # System Prompt
        sys_msg = SystemMessage(content=system)
        
        # Clean up old system messages
        filtered_msgs = [m for m in msgs if not isinstance(m, SystemMessage)]
        input_msgs = [sys_msg] + filtered_msgs

        # G·ªçi LLM
        response = model_with_tools.invoke(input_msgs)
        
        # Log h√†nh vi Agent
        if response.tool_calls:
            logger.info(f"üõ†Ô∏è Agent quy·∫øt ƒë·ªãnh g·ªçi Tool: {response.tool_calls[0]['name']}")
        else:
            logger.info("üß† Agent ph·∫£n h·ªìi tr·ª±c ti·∫øp (Kh√¥ng d√πng Tool).")
            
        return {"messages": [response]}

    # Graph Setup
    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", ToolNode(tools))

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", tools_condition)
    workflow.add_edge("tools", "agent")

    app = workflow.compile()

    # --- ADAPTER (Gi·ªØ nguy√™n ƒë·ªÉ t∆∞∆°ng th√≠ch API c≈©) ---
    class GraphAgentExecutorAdapter:
        """Thin adapter exposing a simple `.invoke` API over a LangGraph app.

        Use this to integrate with existing HTTP handlers or CLI code where
        you want to pass a dict containing `chat_history` and `input`, and
        receive a dict with `output`.
        """
        def __init__(self, graph):
            """Store the compiled graph for later invocations.

            Parameters
            ----------
            graph : Any
                The compiled LangGraph application.
            """
            self.graph = graph

        def invoke(self, inputs: dict):
            """Run a single conversational step through the graph.

            Parameters
            ----------
            inputs : dict
                Dict with optional `chat_history` (list of dicts with `type`
                and `content`) and `input` (latest user message).

            Returns
            -------
            dict
                Dict with a single key `output` containing the assistant's
                message text from the last response.

            Examples
            --------
            ````python
            adapter = GraphAgentExecutorAdapter(app)
            resp = adapter.invoke({
                "chat_history": [{"type": "user", "content": "hi"}],
                "input": "Tell me campus news"
            })
            print(resp["output"])  # assistant response
            ````
            """
            # Chuy·ªÉn ƒë·ªïi chat_history t·ª´ dict sang object LangChain
            msg_list = []
            for m in inputs.get("chat_history", []):
                if m.get("type") == "user":
                    msg_list.append(HumanMessage(content=m.get("content")))
                else:
                    msg_list.append(AIMessage(content=m.get("content")))
            
            if inputs.get("input"):
                msg_list.append(HumanMessage(content=inputs.get("input")))

            result = self.graph.invoke({"messages": msg_list})
            # L·∫•y tin nh·∫Øn AI cu·ªëi c√πng
            final_ai_msg = [m for m in result["messages"] if isinstance(m, AIMessage)][-1]
            return {"output": final_ai_msg.content}

    chatbot_agent_executor = GraphAgentExecutorAdapter(app)
else:
    logger.critical("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Agent do thi·∫øu Chat Model!")
    chatbot_agent_executor = None